---
layout: post
title:  "Journal Club: Heide & Gr√ºnwald (2020)"
author: Raphael
categories: [ journal club ]
image: <> paper title
featured: false
hidden: false
---

A summary and some questions for group discussion.

<br>

### Overview
- [Background](#background)
- [Summary and conclusion](#summary-conclusion)

<br>
<a id='background'></a>

### Background

Frequentist null-hypothesis significance testing (NHST), although pervasive in psychological research, has been heavily criticized for its role in the reproducibility crisis. In particular, NHST is prone to overestimating effect sizes and inflating false positive rates, in a phenomenon known as p-hacking. One way this can happen is through a practice known as "optional stopping".

Optional stopping refers to the practice of "looking at the results so far to decide whether to gather more data". Reasons for doing so are often ones of saving time, energy, and money. If after collecting 10 participants it is clear that there is no effect, one can save themselves the trouble of wasting money collecting more. Similarly, if one collects 10 participants and sees that the data is *almost* significant, one can collect more data until it is.

**EXECPT**, doing so is a massive violation of the assumptions of null-hypothesis testing, and stopping as soon an effect becomes significant massively increases the false positive rate. For example, Simmons et al. (2011) was able to demonstrate a false positive rate as high as 22% (!!) if one peaked at the data to see if it was significant, and stopping when it was.

----

### The Current Paper

Nevertheless, optional stopping is an extremely desirable property of any kind of statistical test for the reasons mentioned above. Why waste valuable time collecting more data when it is clear than an effect is either never going to materialize or is already clearly evident? The critical question of the current paper is whether or not optional stopping is a problem for Bayesians (spoiler if you've read the title... it is). More specifically, the paper explores not just if, but **when** optional stopping is a problem. It is a direct response to Rouder 2014 (title: Optional stopping: No problem for Bayesians)
